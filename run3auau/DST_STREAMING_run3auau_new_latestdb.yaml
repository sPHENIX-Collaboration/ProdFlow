#__________________________________________________________________________________________________________________________________________
DST_STREAMING_EVENT_run3auau_streams:

   params:
     name:       DST_STREAMING_EVENT_$(streamname)_run3auau
     build:      new
     build_name: new
     dbtag:      nocdbtag
     version:    0
     logbase :   $(name)_$(build)_$(tag)_$(version)-$INT(run,{RUNFMT})-$INT(seg,{SEGFMT})
     outbase :   $(name)_$(build)_$(tag)_$(version)
     script  :   run_parallel_streams.sh
     payload :   ./ProdFlow/run3auau/streaming/
     neventsper:  10000
     comment :    "---"
     rsync   : "./ProdFlow/run3auau/streaming/*"
     mem     :   4000MB
     #mnrun   :   63758
     mnrun   : 63360
     mxrun   :   98999

   # Generates a set of temporary tables which may be utilized in the input query below.
   # The name of the temp table is the dictionary key specified in each list element.  (e.g. runnumbers)
   import_tables:
     #- runselection:
     #    db: daqdb
     #    query: |-
     #      select runnumber from run where runnumber>={mnrun} and runnumber<={mxrun} and runtype in ( 'cosmics', 'physics', 'beam', 'calib', 'dryrun', 'line_laser' )
     #      {run_condition}

     # Get the list of runs taken, and for each run, the list of daqhosts for the run...
     - runselection:
         db: daq
         query: |-
           with allruns as (
           select
               hostinfo.runnumber,
               string_agg(
                  hostinfo.hostname||'_'||hostinfo.serverid,' ' order by hostname,serverid
               ) as hosts,
               count( hostinfo.serverid ) filter (where hostinfo.hostname like 'ebdc%' and hostinfo.hostname!='ebdc39' ) as nendpoint,
               run.runtype
           from
              hostinfo join run on run.runnumber=hostinfo.runnumber
           where
              run.runnumber>={mnrun} and
              run.runnumber<={mxrun} and
              (
                 hostinfo.hostname like 'ebdc%' or
                 hostinfo.hostname like 'intt%' or
                 hostinfo.hostname like 'mvtx%'
              ) and
              runtype in ( 'physics', 'beam', 'calib', 'line_laser' )
           group by
              hostinfo.runnumber, run.runtype
           )
           select * from allruns where true {run_condition}           
            
      
   input:
      db: raw
      lfn2pfn: lfn2lfn
      query: |-
        with config as (
        select {mnrun} as firstrun,
               {mxrun} as lastrun
        ),
        alldatasets as (
        select * from datasets,config where runnumber>=config.firstrun and runnumber<=config.lastrun and runnumber in ( select runnumber from runselection )
        ),
        allfiles as (
        select alldatasets.*,files.* from alldatasets join files on files.lfn=alldatasets.filename
        ),
        allstreams as (
        select runnumber,
            filename as streamfile,
            replace(regexp_replace(daqhost,'(intt|mvtx|ebdc)(\d+)$','\1\2:0'),':','_') as streamname,
            full_host_name,
            dataset as runtype
        from allfiles    
            where
             daqhost in ( 'ebdc00','ebdc01','ebdc02','ebdc03','ebdc04','ebdc05',
                          'ebdc06','ebdc07','ebdc08','ebdc09','ebdc10','ebdc11',
                          'ebdc12','ebdc13','ebdc14','ebdc15','ebdc16','ebdc17',
                          'ebdc18','ebdc19','ebdc20','ebdc21','ebdc22','ebdc23',
                          'ebdc39',
                          'ebdc00:1','ebdc01:1','ebdc02:1','ebdc03:1','ebdc04:1','ebdc05:1',
                          'ebdc06:1','ebdc07:1','ebdc08:1','ebdc09:1','ebdc10:1','ebdc11:1',
                          'ebdc12:1','ebdc13:1','ebdc14:1','ebdc15:1','ebdc16:1','ebdc17:1',
                          'ebdc18:1','ebdc19:1','ebdc20:1','ebdc21:1','ebdc22:1','ebdc23:1',
                          'ebdc39:1',                          
                          'intt0','intt1','intt2','intt3','intt4','intt5','intt6','intt7',
                          'mvtx0','mvtx1','mvtx2','mvtx3','mvtx4','mvtx5'
                          )
                  and
                  full_host_name='lustre'
                  ),
        allgl1 as (
                  select runnumber,
                  filename as streamfile,
                  upper( daqhost ) as streamname,
                  full_host_name,
                  dataset as runtype
                  from allfiles     
                  where
                  daqhost='gl1daq' 
                  and
                  full_host_name='lustre'
                  ),
         alljobs1 as (
                  select 'rawdata' as source,
                  runnumber,
          0 as segment,
          string_agg( distinct streamfile, ' ' ) as files,
          'na' as fileranges,
          streamname,
          runtype
          from allstreams
          group by runnumber,allstreams.streamname,runtype
          ),
          alljobs2 as (
          select 'rawdata' as source,
          runnumber,
          0 as segment,
          string_agg( distinct streamfile, ' ' ) as gl1files,
          'na' as gl1fileranges
          from allgl1
          group by runnumber,allgl1.streamname,runtype
          ),
          alljobs as (
          select alljobs1.source,
          alljobs1.runnumber,
          alljobs1.segment,
          files||' '||gl1files as files,
          alljobs1.fileranges, 
          alljobs1.streamname,
          alljobs1.runtype as runtype
          from alljobs1 join alljobs2 on alljobs1.runnumber=alljobs2.runnumber
          )
          select *,runselection.nendpoint from alljobs join runselection on alljobs.runnumber=runselection.runnumber





   #
   # Again I note the need to ensure that the arguments are properly specified given the
   # definition of the payload script.
   #
   job:
     batch_name            : "$(name)_$(build)_$(tag)_$(version)-singlestreams"
     arguments             : "$(nevents) {outbase} {logbase} $(run) $(seg) $(outdir) $(buildarg) $(tag) $(inputs) $(ranges) {neventsper} {logdir} $(nendpoint) {histdir} {PWD} {rsync}"
     output_destination    : '{logdir}'
     log                   : '{condor}/{logbase}.condor'
     priority : '3900'
     request_xferslots: '0'



#__________________________________________________________________________________________________________________________________________
DST_TRKR_CLUSTER_run3auau_streams:

   params:
     name:       DST_TRKR_CLUSTER_run3auau
     build:      new
     build_name: new
     dbtag:      2025p001
     version:    0
     logbase :   $(name)_$(build)_$(tag)_$(version)-$INT(run,{RUNFMT})-$INT(seg,{SEGFMT})
     outbase :   $(name)_$(build)_$(tag)_$(version)
     script  :   run_singlejob0.sh
     payload :   ./ProdFlow/run3auau/TrackingProduction/
     neventsper:      10000
     comment :    "---"
     rsync   : "./ProdFlow/run3auau/TrackingProduction/*"
     mem     :   8192MB
     mnrun   : 63758
     mxrun   : 98999
     #dstin   : 'DST_STREAMING_EVENT_[a-z]%_run3auau'
     dstin   : '^DST_STREAMING_EVENT_[a-z0-9]+_[01]_run3auau'
     dataset : 'new_nocdbtag_v000'

   import_tables:
     # Get the list of runs taken, and for each run, the list of daqhosts for the run...
     - hosttable:
         db: daq
         query: |-
           with allruns as (
           select
               hostinfo.runnumber,
               string_agg(
                  hostinfo.hostname||'_'||hostinfo.serverid,' ' order by hostname,serverid
               ) as hosts,
               count( hostinfo.serverid ) filter (where hostinfo.hostname like 'ebdc%' ) as ebdctype,
               run.runtype
           from
              hostinfo join run on run.runnumber=hostinfo.runnumber
           where
              run.runnumber>={mnrun} and
              run.runnumber<={mxrun} and
              (
                 hostinfo.hostname like 'ebdc%' or
                 hostinfo.hostname like 'intt%' or
                 hostinfo.hostname like 'mvtx%'
              ) and
              runtype in ( 'physics', 'beam', 'cosmics', 'calib', 'dryrun' )
           group by
              hostinfo.runnumber, run.runtype
           )
           select * from allruns where true {run_condition}

       
   input:
      db: fc
      lfn2pfn: lfn2lfn
      query: |-
        
         with allruns as (
         select
                'filecatalog/datasets'         as source        ,
                runnumber                                       ,
                segment                                         ,
                string_agg( distinct filename, ' ' ) as files   ,
                'NA'  as fileranges                             ,
                count(distinct filename) as ninputs             ,
                string_agg( distinct regexp_replace(filename,'DST_STREAMING_EVENT_([a-z0-9]+_[01])_run3auau[a-z0-9_\-.]+','\1'),' ') as hosts
                    
                
         from
                datasets
         where
                dsttype~'{dstin}' and dataset~'{dataset}'
                {run_condition}
                and runnumber>={mnrun}
                and runnumber<={mxrun}

         group by runnumber, segment

         order by runnumber {limit_condition}
         )

         select allruns.*,hosttable.runtype,hosttable.ebdctype from allruns join hosttable on allruns.runnumber=hosttable.runnumber and allruns.hosts=hosttable.hosts



   #
   # Again I note the need to ensure that the arguments are properly specified given the
   # definition of the payload script.
   #
   job:
     arguments             : "$(nevents) {outbase} {logbase} $(run) $(seg) $(outdir) $(buildarg) $(tag) $(inputs) $(ranges) {neventsper} {logdir} {comment} {histdir} {PWD} {rsync}"
     output_destination    : '{logdir}'
     log                   : '{condor}/{logbase}.condor'
     priority : '3900'
     request_xferslots: '0'

     

#__________________________________________________________________________________________________________________________________________
DST_TRKR_SEED_run3auau_streams:

   params:
     name:       DST_TRKR_SEED_run3auau
     build:      new
     build_name: new
     dbtag:      2025p001
     version:    0
     logbase :   $(name)_$(build)_$(tag)_$(version)-$INT(run,{RUNFMT})-$INT(seg,{SEGFMT})
     outbase :   $(name)_$(build)_$(tag)_$(version)
     script  :   run_jobA.sh
     payload :   ./ProdFlow/run3auau/TrackingProduction/
     neventsper:      10000
     comment :    "---"
     rsync   : "./ProdFlow/run3auau/TrackingProduction/*"
     mem     :   4000MB
     mnrun   : 63758
     mxrun   : 98999
     dstin   : 'DST_TRKR_CLUSTER_run3auau'
     dataset : 'new_2025p001_v000'

   import_tables:
     - runselection:
         db: daqdb
         query: |-
           with allruns as (
           select
               hostinfo.runnumber,string_agg(hostinfo.hostname,' ' order by hostname) as hosts,run.runtype
           from
              hostinfo join run on run.runnumber=hostinfo.runnumber
           where
              run.runnumber>={mnrun} and
              run.runnumber<={mxrun} and
              (
                 hostinfo.hostname like 'ebdc%' or
                 hostinfo.hostname like 'intt%' or
                 hostinfo.hostname like 'mvtx%'
              ) and
              runtype in ( 'physics', 'beam', 'cosmics', 'calib', 'dryrun' ) group by hostinfo.runnumber, run.runtype
              )
              select * from allruns where true {run_condition}

   input:
      db: fc
      query: |-
         with allruns as (
         select
                'filecatalog/datasets'         as source        ,
                runnumber                                       ,
                segment                                         ,
                string_agg( distinct filename, ' ' ) as files   ,
                'NA'  as fileranges,
                count(distinct filename) as ninputs
         from
                datasets
         where
                dsttype like '{dstin}' and dataset='{dataset}'
                {run_condition}
                and runnumber>={mnrun}
                and runnumber<={mxrun}

         group by runnumber, segment

         order by runnumber {limit_condition}
         )

         select allruns.*,runselection.runtype from allruns join runselection on allruns.runnumber=runselection.runnumber where allruns.ninputs>0 order by runnumber, segment


   #
   # Again I note the need to ensure that the arguments are properly specified given the
   # definition of the payload script.
   #
   job:
     arguments             : "$(nevents) {outbase} {logbase} $(run) $(seg) $(outdir) $(buildarg) $(tag) $(inputs) $(ranges) {neventsper} {logdir} {comment} {histdir} {PWD} {rsync}"
     output_destination    : '{logdir}'
     log                   : '{condor}/{logbase}.condor'
     #accounting_group      : "group_sphenix.mdc2"
     #accounting_group_user : "sphnxpro"
     priority : '3900'
     request_xferslots: '0'

     


#__________________________________________________________________________________________________________________________________________
DST_TRKR_TRACKS_run3auau_streams:

   params:
     name:       DST_TRKR_TRACKS_run3auau
     build:      new
     build_name: new
     dbtag:      2025p001
     version:    0
     logbase :   $(name)_$(build)_$(tag)_$(version)-$INT(run,{RUNFMT})-$INT(seg,{SEGFMT})
     outbase :   $(name)_$(build)_$(tag)_$(version)
     script  :   run_jobC.sh
     payload :   ./ProdFlow/run3auau/TrackingProduction/
     neventsper:      10000
     comment :    "---"
     rsync   : "./ProdFlow/run3auau/TrackingProduction/*"
     mem     :   4000MB
     mnrun   : 63758
     mxrun   : 98999
     dstin1   : 'DST_TRKR_CLUSTER_run3auau'
     dataset1 : 'new_2025p001_v000'
     dstin2   : 'DST_TRKR_SEED_run3auau'
     dataset2 : 'new_2025p001_v000'

   import_tables:
     - runselection:
         db: daqdb
         query: |-
           with allruns as (
           select
               hostinfo.runnumber,string_agg(hostinfo.hostname,' ' order by hostname) as hosts,run.runtype
           from
              hostinfo join run on run.runnumber=hostinfo.runnumber
           where
              run.runnumber>={mnrun} and
              run.runnumber<={mxrun} and
              (
                 hostinfo.hostname like 'ebdc%' or
                 hostinfo.hostname like 'intt%' or
                 hostinfo.hostname like 'mvtx%'
              ) and
              runtype in ( 'physics', 'beam', 'cosmics', 'calib', 'dryrun' ) group by hostinfo.runnumber, run.runtype
              )
              select * from allruns where true {run_condition}

   input:
      db: fc
      query: |-
         with allruns as (
         select
                'filecatalog/datasets'         as source        ,
                runnumber                                       ,
                segment                                         ,
                string_agg( distinct filename, ' ' ) as files   ,
                'NA'  as fileranges,
                count(distinct filename) as ninputs
         from
                datasets
         where
                (
                   (dsttype='{dstin1}' and dataset='{dataset1}') or
                   (dsttype='{dstin2}' and dataset='{dataset2}')
                )
                {run_condition}
                and runnumber>={mnrun}
                and runnumber<={mxrun}

         group by runnumber, segment

         having (

            sum( case when (dsttype = '{dstin1}' and dataset='{dataset1}')     then 1 else 0 end )>0    and
            sum( case when (dsttype = '{dstin2}' and dataset='{dataset2}')     then 1 else 0 end )>0 

         )         

         order by runnumber {limit_condition}
         )

         select allruns.*,runselection.runtype from allruns join runselection on allruns.runnumber=runselection.runnumber where allruns.ninputs>0 order by runnumber, segment         


   #
   # Again I note the need to ensure that the arguments are properly specified given the
   # definition of the payload script.
   #
   job:
     arguments             : "$(nevents) {outbase} {logbase} $(run) $(seg) $(outdir) $(buildarg) $(tag) $(inputs) $(ranges) {neventsper} {logdir} {comment} {histdir} {PWD} {rsync}"
     output_destination    : '{logdir}'
     log                   : '{condor}/{logbase}.condor'
     #accounting_group      : "group_sphenix.mdc2"
     #accounting_group_user : "sphnxpro"
     priority : '3900'
     request_xferslots: '0'

     
     



























#__________________________________________________________________________________________________________________________________________
#__________________________________________________________________________________________________________________________________________
#__________________________________________________________________________________________________________________________________________
deprecated_DST_STREAMING_EVENT_run3calib_streams:

   params:
     name:       DST_STREAMING_EVENT_$(streamname)_run3calib
     build:      new
     build_name: new
     dbtag:      nocdbtag
     version:    0
     logbase :   $(name)_$(build)_$(tag)_$(version)-$INT(run,{RUNFMT})-$INT(seg,{SEGFMT})
     outbase :   $(name)_$(build)_$(tag)_$(version)
     script  :   run_parallel_streams.sh
     payload :   ./ProdFlow/run3auau/streaming/
     neventsper:  10000
     comment :    "---"
     rsync   : "./ProdFlow/run3auau/streaming/*"
     mem     :   4000MB

   input:
      db: daqdb
      direct_path: /sphenix/lustre01/sphnxpro/{mode}/*/*/
      query: |-
         with run2pp as (
              select 63758 as firstrun,
                     98999 as lastrun
         ),
         streams as (select distinct split_part( split_part(filename,'/',-1), '-', 1 ) as streamfile, hostname                     from filelist,run2pp where runnumber>=run2pp.firstrun and runnumber<=run2pp.lastrun),

         intt_stream as (
              select streamfile, hostname as names from streams where hostname in ( 'intt0', 'intt1', 'intt2', 'intt3', 'intt4', 'intt5', 'intt6', 'intt7' )
         ),
         mvtx_stream as (
              select streamfile, hostname as names from streams where hostname in ( 'mvtx0',  'mvtx1',  'mvtx2',  'mvtx3',  'mvtx4',  'mvtx5' )
         ),
         tpot_stream as ( 
              select streamfile, 'TPOT' as names from streams where hostname in ( 'ebdc39' ) 
         ),
         tpc_stream as (
              select streamfile, replace( hostname, 'ebdc', 'TPC' ) as streamname from streams where hostname in ( 'ebdc00', 'ebdc01', 'ebdc02', 'ebdc03', 'ebdc04', 'ebdc05', 'ebdc06', 'ebdc07', 'ebdc08', 'ebdc09', 
                                                                                    'ebdc10', 'ebdc11', 'ebdc12', 'ebdc13', 'ebdc14', 'ebdc15', 'ebdc16', 'ebdc17', 'ebdc18', 'ebdc19', 
                                                                                    'ebdc20', 'ebdc21', 'ebdc22', 'ebdc23' ) 
         ),
         all_streams as ( 
             select * from tpc_stream
                 union
             select * from intt_stream
                 union 
             select * from mvtx_stream
                 union
             select * from tpot_stream
         ),

         all_jobs as (

           select 'daqdb/filelist' as source, runnumber, 0 as segment, string_agg( distinct split_part(filename,'/',-1), ' ' ) as files, 'na' as fileranges, UPPER(all_streams.streamname) as streamname, all_streams.streamfile
 
           from filelist, run2pp, all_streams

           where 
           transferred_to_sdcc='true' and
           runnumber>=run2pp.firstrun and runnumber<=run2pp.lastrun and
           (
               (filename  similar to '/bbox%/GL1_(calib)%.evt'   and lastevent>2 ) or
               (filename  similar to '%'||all_streams.streamfile||'%' and filename similar to '%(calib)%' )           
           )

           {run_condition}

           group by runnumber,all_streams.streamfile,all_streams.streamname

           having
                max(lastevent)>1000          and
                sum( case when filename similar to '/bbox%/GL1_(calib)%.evt' then 1 else 0 end )>0 and
                (
                   sum( case when filename like '/bbox%/TPC%'   then 1 else 0 end )>0 or
                   sum( case when filename like '/bbox%/%intt%' then 1 else 0 end )>0 or
                   sum( case when filename like '/bbox%/%mvtx%' then 1 else 0 end )>0 or
                   sum( case when filename like '/bbox%/TPOT%'  then 1 else 0 end )>0 
                )

           )        

           select * from all_jobs 



              ;


   #
   # Again I note the need to ensure that the arguments are properly specified given the
   # definition of the payload script.
   #
   job:
     batch_name            : "$(name)_$(build)_$(tag)_$(version)-singlestreams"
     arguments             : "$(nevents) {outbase} {logbase} $(run) $(seg) $(outdir) $(buildarg) $(tag) $(inputs) $(ranges) {neventsper} {logdir} {comment} {histdir} {PWD} {rsync}"
     output_destination    : '{logdir}'
     log                   : '{condor}/{logbase}.condor'
     #accounting_group      : "group_sphenix.mdc2"
     #accounting_group_user : "sphnxpro"
     priority : '13900'
     request_xferslots: '0'

#__________________________________________________________________________________________________________________________________________
deprecated_DST_TRKR_CLUSTER_run3calib_streams:

   params:
     name:       DST_TRKR_CLUSTER_run3calib
     build:      new
     build_name: new
     dbtag:      2025p001
     version:    0
     logbase :   $(name)_$(build)_$(tag)_$(version)-$INT(run,{RUNFMT})-$INT(seg,{SEGFMT})
     outbase :   $(name)_$(build)_$(tag)_$(version)
     script  :   run_singlejob0.sh
     payload :   ./ProdFlow/run3auau/TrackingProduction/
     neventsper:      10000
     comment :    "---"
     rsync   : "./ProdFlow/run3auau/TrackingProduction/*"
     mem     :   8192MB
     mnrun   : 63758
     mxrun   : 98999
     dstin   : 'DST_STREAMING_EVENT_%_run3calib'
     dataset : 'new_nocdbtag_v000'

   input:
      db: fc
      query: |-
         with allruns as (
         select
                'filecatalog/datasets'         as source        ,
                runnumber                                       ,
                segment                                         ,
                string_agg( distinct filename, ' ' ) as files   ,
                'NA'  as fileranges,
                count(distinct filename) as ninputs
         from
                datasets
         where
                dsttype like '{dstin}' and dataset='{dataset}'
                {run_condition}
                and runnumber>={mnrun}
                and runnumber<={mxrun}

         group by runnumber, segment

         order by runnumber {limit_condition}
         )

         select * from allruns where allruns.ninputs>0 order by runnumber, segment


   #
   # Again I note the need to ensure that the arguments are properly specified given the
   # definition of the payload script.
   #
   job:
     arguments             : "$(nevents) {outbase} {logbase} $(run) $(seg) $(outdir) $(buildarg) $(tag) $(inputs) $(ranges) {neventsper} {logdir} {comment} {histdir} {PWD} {rsync}"
     output_destination    : '{logdir}'
     log                   : '{condor}/{logbase}.condor'
     #accounting_group      : "group_sphenix.mdc2"
     #accounting_group_user : "sphnxpro"
     priority : '13900'
     request_xferslots: '0'     








POST_PROCESSING_STREAMING_EVENT_run3auau:
  # Logfile compression & etc...
  params:
    name: POST_STREAMING_EVENT_$(streamname)_run3auau
    build: new
    build_name: new
    dbtag: nocdbtag
    version: 0
    logbase :   $(name)_$(build)_$(tag)_$(version)-$INT(run,{RUNFMT})-$INT(seg,{SEGFMT})
    outbase :   $(name)_$(build)_$(tag)_$(version)
    script  :   run_post_processing.sh  
    payload :   ./ProdFlow/run3auau/postprocess/
    rsync :     "./ProdFlow/run3auau/postprocess/*"
    mnrun: 60000
    mxrun: 98999
    neventsper: 0

     # Get the list of runs taken, and for each run, the list of daqhosts for the run...
  import_tables:
    - runselection:
        db: daqdb
        query: |-
          select runnumber,runtype from run where runnumber>={mnrun} and runnumber<={mxrun} and runtype in ( 'cosmics', 'physics', 'beam', 'calib', 'dryrun', 'line_laser' )
          {run_condition}

  input:
    db: statusw
    lfn2pfn: lfn2lfn
    query: |-
      with jobs as (
         select 'production' as source,
         run,
         0 as segment,
         (regexp_matches(dsttype,'([a-z0-9]+|[a-z0-9]+_[01])_run3'))[1] as streamname,
         dstfile||'.out' as files,
         dstfile||':'||status as fileranges
      from
         production_status
      where
         run>={mnrun} and run<={mxrun} and status>'running' and dstfile similar to 'DST_STREAMING_EVENT_(ebdc|intt|mvtx)%_run3auau_new_nocdbtag%'
      )
      select jobs.*,runselection.* from jobs join runselection on run=runnumber

      where true {run_condition}

  job:
    batch_name            : "POST_STREAMING_EVENT_$(build)_$(tag)_$(version)"
    arguments             : "$(nevents) {outbase} {logbase} $(run) $(seg) $(outdir) $(buildarg) $(tag) $(inputs) $(ranges) {neventsper} {logdir} $(nendpoint) {histdir} {PWD} {rsync}"
    output_destination    : '{logdir}'
    log                   : '{condor}/{logbase}.condor'
    priority : '3900'
    request_xferslots: '0'     


POST_PROCESSING_TRKR_CLUSTER_run3auau:
  # Logfile compression & etc...
  params:
    name: POST_TRKR_CLUSTER_run3auau
    build: new
    build_name: new
    dbtag: 2025p001
    version: 0
    logbase :   $(name)_$(build)_$(tag)_$(version)-$INT(run,{RUNFMT})-$INT(seg,{SEGFMT})
    outbase :   $(name)_$(build)_$(tag)_$(version)
    script  :   run_post_processing.sh  
    payload :   ./ProdFlow/run3auau/postprocess/
    rsync :     "./ProdFlow/run3auau/postprocess/*"
    mnrun: 60000
    mxrun: 98999
    neventsper: 0

     # Get the list of runs taken, and for each run, the list of daqhosts for the run...
  import_tables:
    - runselection:
        db: daqdb
        query: |-
          select runnumber,runtype from run where runnumber>={mnrun} and runnumber<={mxrun} and runtype in ( 'cosmics', 'physics', 'beam', 'calib', 'dryrun', 'line_laser' )
          {run_condition}

  input:
    db: statusw
    lfn2pfn: lfn2lfn
    query: |-
      with jobs as (
         select 'production' as source,
         run,
         segment,
         dstfile||'.out' as files,
         dstfile||':'||status as fileranges
      from
         production_status
      where
         run>={mnrun} and run<={mxrun} and status>'running' and dstfile similar to 'DST_TRKR_CLUSTER%_run3auau_new_2025p001%'
      )
      select jobs.*,runselection.* from jobs join runselection on run=runnumber

      where true {run_condition}

  job:
    batch_name            : "POST_TRKR_CLUSTER_$(build)_$(tag)_$(version)"
    arguments             : "$(nevents) {outbase} {logbase} $(run) $(seg) $(outdir) $(buildarg) $(tag) $(inputs) $(ranges) {neventsper} {logdir} $(nendpoint) {histdir} {PWD} {rsync}"
    output_destination    : '{logdir}'
    log                   : '{condor}/{logbase}.condor'
    priority : '3900'
    request_xferslots: '0'

POST_PROCESSING_TRKR_SEED_run3auau:
  # Logfile compression & etc...
  params:
    name: POST_TRKR_SEED_run3auau
    build: new
    build_name: new
    dbtag: 2025p001
    version: 0
    logbase :   $(name)_$(build)_$(tag)_$(version)-$INT(run,{RUNFMT})-$INT(seg,{SEGFMT})
    outbase :   $(name)_$(build)_$(tag)_$(version)
    script  :   run_post_processing.sh  
    payload :   ./ProdFlow/run3auau/postprocess/
    rsync :     "./ProdFlow/run3auau/postprocess/*"
    mnrun: 60000
    mxrun: 98999
    neventsper: 0

     # Get the list of runs taken, and for each run, the list of daqhosts for the run...
  import_tables:
    - runselection:
        db: daqdb
        query: |-
          select runnumber,runtype from run where runnumber>={mnrun} and runnumber<={mxrun} and runtype in ( 'cosmics', 'physics', 'beam', 'calib', 'dryrun', 'line_laser' )
          {run_condition}

  input:
    db: statusw
    lfn2pfn: lfn2lfn
    query: |-
      with jobs as (
         select 'production' as source,
         run,
         segment,
         dstfile||'.out' as files,
         dstfile||':'||status as fileranges
      from
         production_status
      where
         run>={mnrun} and run<={mxrun} and status>'running' and dstfile similar to 'DST_TRKR_SEED_run3auau_new_2025p001%'
      )
      select jobs.*,runselection.* from jobs join runselection on run=runnumber

      where true {run_condition}

  job:
    batch_name            : "POST_TRKR_SEED_$(build)_$(tag)_$(version)"
    arguments             : "$(nevents) {outbase} {logbase} $(run) $(seg) $(outdir) $(buildarg) $(tag) $(inputs) $(ranges) {neventsper} {logdir} $(nendpoint) {histdir} {PWD} {rsync}"
    output_destination    : '{logdir}'
    log                   : '{condor}/{logbase}.condor'
    priority : '3900'
    request_xferslots: '0'         


POST_PROCESSING_TRKR_TRACKS_run3auau:
  # Logfile compression & etc...
  params:
    name: POST_TRKR_TRACKS_run3auau
    build: new
    build_name: new
    dbtag: 2025p001
    version: 0
    logbase :   $(name)_$(build)_$(tag)_$(version)-$INT(run,{RUNFMT})-$INT(seg,{SEGFMT})
    outbase :   $(name)_$(build)_$(tag)_$(version)
    script  :   run_post_processing.sh  
    payload :   ./ProdFlow/run3auau/postprocess/
    rsync :     "./ProdFlow/run3auau/postprocess/*"
    mnrun: 60000
    mxrun: 98999
    neventsper: 0

     # Get the list of runs taken, and for each run, the list of daqhosts for the run...
  import_tables:
    - runselection:
        db: daqdb
        query: |-
          select runnumber,runtype from run where runnumber>={mnrun} and runnumber<={mxrun} and runtype in ( 'cosmics', 'physics', 'beam', 'calib', 'dryrun', 'line_laser' )
          {run_condition}

  input:
    db: statusw
    lfn2pfn: lfn2lfn
    query: |-
      with jobs as (
         select 'production' as source,
         run,
         segment,
         dstfile||'.out' as files,
         dstfile||':'||status as fileranges
      from
         production_status
      where
         run>={mnrun} and run<={mxrun} and status>'running' and dstfile similar to 'DST_TRKR_TRACKS%_run3auau_new_2025p001%'
      )
      select jobs.*,runselection.* from jobs join runselection on run=runnumber

      where true {run_condition}

  job:
    batch_name            : "POST_TRKR_TRACKS_$(build)_$(tag)_$(version)"
    arguments             : "$(nevents) {outbase} {logbase} $(run) $(seg) $(outdir) $(buildarg) $(tag) $(inputs) $(ranges) {neventsper} {logdir} $(nendpoint) {histdir} {PWD} {rsync}"
    output_destination    : '{logdir}'
    log                   : '{condor}/{logbase}.condor'
    priority : '3900'
    request_xferslots: '0'         
